{
  "Quote 1": {
    "quote": "Historically, it has been unclear whether unlearning techniques are removing information from the model weights or just making it harder to access. To disentangle these two objectives, we propose an adversarial evaluation method to test for the removal of information from model weights",
    "similarity_score": 1.0
  },
  "Quote 2": {
    "quote": "The attack we study in this work is Retraining on T (RTT) which is illustrated in Figure 1: the attacker trains the unlearned model on the facts they have access to, T. After performing RTT, we can use accuracy on V to approximate the mutual information introduced in section 3.1: if the accuracy on V is high, mutual information has to be high. If the accuracy is close to random chance, mutual information is probably low.",
    "similarity_score": 1.0
  },
  "Quote 3": {
    "quote": "Using fine-tuning on the accessible facts can recover 88% of the pre-unlearning accuracy when applied to current unlearning methods for information learned during pretraining, revealing the limitations of these methods in removing information from the model weights.",
    "similarity_score": 1.0
  },
  "Quote 4": {
    "quote": "For all methods, RTT recovers the forget accuracy close to its original level for pretrained information (Years, MMLU, and WMDP-Deduped datasets), which suggests that most of the information was hidden, not removed from the weights.",
    "similarity_score": 1.0
  },
  "Quote 5": {
    "quote": "1. Gradient Difference (GD) (Liu et al., 2022): Gradient Ascent on the forget dataset and Gradient Descent on the retain dataset. LossGD = \u2212LossForget + \u03b1 \u2217 LossRetain Where \u03b1 is the retain coefficient. We note that using a retain coefficient of 0 corresponds to using the Gradient Ascent unlearning method. 2. RMU (Li et al., 2024b): An unlearning technique that perturbs the activations of the model in a subset of the models' layers for harmful prompts while preserving the activations for non-harmful prompts. 3. Random Incorrect Answer (RIA): For each question with multiple choice answers, we create a plain-text formatted datapoint for each incorrect choice and perform gradient descent on these texts.",
    "similarity_score": 0.3606936416184971
  },
  "Quote 6": {
    "quote": "1. The dataset has little shared information among facts: Learning some of the facts should not help in learning the rest if the information is not already present in the weights. 2. Models perform well on the dataset before unlearning: This means we do not need to fine-tune the models on the information, which may result in a different response to unlearning compared to information learned in pretraining. 3. The data resembles what unlearning is used for in practice.",
    "similarity_score": 1.0
  },
  "Quote 7": {
    "quote": "RTT does not recover information for the fine-tuned dataset (Random Birthdays); we still get a high recovery rate for RMU, but a recovery rate less than 35% for GD and RIA. We suspect that changes introduced by fine-tuning might be relatively superficial and easy to reverse as observed by Jain et al. (2024).",
    "similarity_score": 1.0
  },
  "Quote 8": {
    "quote": "Our results also suggest that unlearning evaluations that measure unlearning robustness on information learned during an additional fine-tuning phase may overestimate robustness compared to evaluations that attempt to unlearn information learned during pretraining.",
    "similarity_score": 1.0
  }
}