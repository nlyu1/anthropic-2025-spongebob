1. gradient difference (gd): gradient ascent on the forget dataset and gradient descent on the retain dataset. lossgd = \u2212lossforget + \u03b1 \u2217 lossretain where \u03b1 is the retain coefficient. we note that using a retain coefficient of 0 corresponds to using the gradient ascent unlearning method. 2. rmu: an unlearning technique that perturbs the activations of the model in a subset of the models' layers for harmful prompts while preserving the activations for nonharmful prompts. 3. random incorrect answer (ria): for each question with multiple choice answers, we create a plaintext formatted datapoint for each incorrect choice and perform gradient descent on these texts.